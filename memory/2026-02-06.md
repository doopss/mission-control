# 2026-02-06

## Model Routing Configuration

Set up intelligent model routing:
- **Default model:** Sonnet 4 (fast, cheap, handles 80% of tasks)
- **Escalation model:** Opus 4.5 (for complex coding, research, reasoning)

**How it works:**
- I (HueleBicho) detect complex tasks and spawn sub-agents with Opus
- Simple stuff stays on Sonnet for speed/cost
- User can override with `/model opus` or `opus:` prefix

**Files changed:**
- `SOUL.md` - Added model routing guidelines
- `openclaw.json` - Changed primary model from Opus to Sonnet, added fallbacks
- `openclaw.json` - Enabled `cacheRetention: "long"` (1-hour cache) for both models

## Prompt Caching Enabled

Configured 1-hour prompt caching for massive cost savings:
- **Cache write:** $3.75/MTok (25% premium, one-time per hour)
- **Cache read:** $0.30/MTok (90% discount!)
- **Perfect for:** Our long system prompts (AGENTS.md, SOUL.md, etc.)
- **Expected savings:** 80-90% on input tokens in multi-turn conversations

OpenClaw automatically caches system prompts, so every message after the first in a 1-hour window gets 90% cheaper input token costs.

## Repos Discussed (not yet installed)

1. **qmd** (github.com/tobi/qmd) - Local semantic search for docs/memory
   - Would reduce token consumption by searching before loading full files
   - Install: `bun install -g github:tobi/qmd`

2. **open-webui** (github.com/open-webui/open-webui) - Local LLM server
   - Could add local models as even cheaper option
   - Runs via Docker + Ollama
